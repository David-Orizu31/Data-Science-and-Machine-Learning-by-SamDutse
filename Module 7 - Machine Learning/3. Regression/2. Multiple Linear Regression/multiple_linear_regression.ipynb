{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiple Linear Regression Notebook**\n",
    "\n",
    "### Teaching Tips for the Module\n",
    "\n",
    "- **Concept**: Multiple Linear Regression models the relationship between one dependent variable and **two or more independent variables**.\n",
    "- **Formula**:\n",
    "  $$ y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n $$\n",
    "- **Dataset Insight**:\n",
    "  - Columns: R&D Spend, Administration, Marketing Spend, State, Profit\n",
    "  - “Profit” is predicted based on various spending areas and location.\n",
    "\n",
    "- **Skills Students Will Learn**:\n",
    "  - Handling categorical data with OneHotEncoding\n",
    "  - Fitting multiple linear regression\n",
    "  - Evaluating model performance using R² score\n",
    "  - Making comparisons and visualizing results\n",
    "\n",
    "1. Manually applies **One-Hot Encoding** on the **State** column (without `ColumnTransformer`),\n",
    "2. Applies **Feature Scaling** using `StandardScaler` to see if normalization helps performance,\n",
    "3. Follows the structure from your previous notebooks.\n",
    "\n",
    "### Key Notes for Students:\n",
    "- **Why `drop_first=True`?** Avoids multicollinearity by removing one dummy column.\n",
    "- **Why Scale the Features?** Standardization helps the model converge faster and can improve accuracy, especially if you plan to extend to algorithms sensitive to feature magnitudes (e.g., gradient-based models).\n",
    "- **Model Evaluation:** Introduced R² score to measure how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loading the Dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separating Features and Target\n",
    "X = dataset.iloc[:, :-1]  # All columns except 'Profit'\n",
    "y = dataset.iloc[:, -1]   # 'Profit' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. One-Hot Encoding the 'State' Column (manually)\n",
    "state = pd.get_dummies(X['State'], drop_first=True)  # Drop first to avoid dummy variable trap\n",
    "X = X.drop('State', axis=1)\n",
    "X = pd.concat([state, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Splitting the Dataset into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training the Multiple Linear Regression Model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Making Predictions\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Comparing Actual vs Predicted\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Evaluating the Model\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Visualization (Optional)\n",
    "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual')\n",
    "plt.scatter(range(len(y_pred)), y_pred, color='red', label='Predicted')\n",
    "plt.title('Actual vs Predicted Profits')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  # Import the main api module\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n",
    "X_opt = X[:, [0,1,2,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That extra column of ones we're adding to `X` is there to represent the **intercept** term in your linear regression model.\n",
    "\n",
    "Think of a simple straight line equation: $\\mathbf{y = mx + c}$.\n",
    "\n",
    "* $\\mathbf{y}$ is your dependent variable.\n",
    "* $\\mathbf{x}$ is your independent variable.\n",
    "* $\\mathbf{m}$ is the slope.\n",
    "* $\\mathbf{c}$ is the y-intercept (the value of $\\mathbf{y}$ when $\\mathbf{x}$ is 0).\n",
    "\n",
    "When you have multiple independent variables, this expands to something like: $\\mathbf{y = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_0}$.\n",
    "\n",
    "That $\\mathbf{\\beta_0}$ is your intercept. To make the matrix calculations work neatly in the background when `statsmodels` fits the model, we represent this intercept as a coefficient multiplied by a column of ones.\n",
    "\n",
    "So, by adding that column of ones to your `X` matrix, you're essentially giving the `OLS` model a dedicated column to learn the intercept term ($\\mathbf{\\beta_0}$). Each row in this column has a value of 1, so when the model calculates the weighted sum of your features, it can also learn a constant offset (the intercept) without needing a separate variable in your original data.\n",
    "\n",
    "**In short: The column of ones is a placeholder in your feature matrix that allows the linear regression model to learn the constant (intercept) term.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
