{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b7a57b",
   "metadata": {},
   "source": [
    "## Evaluating Regression Model Performance\n",
    "\n",
    "## üìò Introduction\n",
    "This notebook provides a comprehensive overview of how to evaluate regression models using common metrics, and offers guidance on how to select the best model depending on your dataset and problem.\n",
    "\n",
    "### Models already covered:\n",
    " - Simple Linear Regression\n",
    " - Multiple Linear Regression with OLS Backward Elimination\n",
    " - Polynomial Regression\n",
    " - Support Vector Regression (SVR)\n",
    " - Decision Tree Regression\n",
    " - Random Forest Regression\n",
    " - XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62df330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 1: Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab23996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Step 2: Load the Dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store models and their predictions\n",
    "models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d91a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìê Step 3: Define Evaluation Function\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e890b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Simple Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "evaluate_model(\"Linear Regression\", y, y_pred)\n",
    "\n",
    "# 2Ô∏è‚É£ Polynomial Regression (Degree 4)\n",
    "poly_reg = PolynomialFeatures(degree=4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "lin_reg2 = LinearRegression()\n",
    "lin_reg2.fit(X_poly, y)\n",
    "y_pred = lin_reg2.predict(X_poly)\n",
    "evaluate_model(\"Polynomial Regression\", y, y_pred)\n",
    "\n",
    "# 3Ô∏è‚É£ Support Vector Regression (SVR)\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_scaled = sc_X.fit_transform(X)\n",
    "y_scaled = sc_y.fit_transform(y).flatten()\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_scaled, y_scaled)\n",
    "y_pred = sc_y.inverse_transform(svr.predict(X_scaled).reshape(-1, 1))\n",
    "evaluate_model(\"SVR\", y, y_pred)\n",
    "\n",
    "# 4Ô∏è‚É£ Decision Tree Regression\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "dtr.fit(X, y)\n",
    "y_pred = dtr.predict(X).reshape(-1, 1)\n",
    "evaluate_model(\"Decision Tree\", y, y_pred)\n",
    "\n",
    "# 5Ô∏è‚É£ Random Forest Regression\n",
    "rfr = RandomForestRegressor(n_estimators=300, random_state=0)\n",
    "rfr.fit(X, y.ravel())\n",
    "y_pred = rfr.predict(X).reshape(-1, 1)\n",
    "evaluate_model(\"Random Forest\", y, y_pred)\n",
    "\n",
    "# 6Ô∏è‚É£ XGBoost Regression\n",
    "xgb = XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=0)\n",
    "xgb.fit(X, y)\n",
    "y_pred = xgb.predict(X).reshape(-1, 1)\n",
    "evaluate_model(\"XGBoost\", y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33750f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Step 4: Display the Evaluation Results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nEvaluation Metrics Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8488bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 5: Visual Comparison\n",
    "results_df.plot(kind='bar', figsize=(12, 6), title=\"Model Evaluation Comparison\")\n",
    "plt.ylabel(\"Error Metrics\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Step 6: k-Fold Cross Validation Example (on Random Forest)\n",
    "scores = cross_val_score(estimator=rfr, X=X, y=y.ravel(), cv=10, scoring='r2')\n",
    "print(f\"\\nRandom Forest 10-Fold CV R2 Mean: {scores.mean():.4f}, Std: {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Model Selection Guide:\n",
    "# - Use Simple/Multiple Linear Regression for problems with linear trends.\n",
    "# - Use Polynomial Regression for curved patterns.\n",
    "# - Use SVR if you suspect non-linear boundaries and want regularization.\n",
    "# - Use Decision Tree when the data has complex splits or abrupt changes.\n",
    "# - Use Random Forest to reduce overfitting of Decision Trees and get better generalization.\n",
    "# - Use XGBoost for high-performance, scalable predictions especially in competitions or large data.\n",
    "\n",
    "# üîß Future Steps: Hyperparameter Tuning\n",
    "# Covered in Part 10 - Model Selection (GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "# üìå Note: Consider model interpretability, computational cost, and overfitting risks when choosing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
